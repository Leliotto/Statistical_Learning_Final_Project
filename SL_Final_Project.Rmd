---
title: "SL_Final_Project"
author: "Davide Fabio"
date: "2025-04-23"
output:
  pdf_document:
    fig_width: 4
    fig_height: 2.5
  html_document:
    df_print: paged
  word_document: default
  html_notebook: default
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
# Opzioni globali per knitr
knitr::opts_chunk$set(echo = FALSE, results = 'show')
# knitr::opts_chunk$set(fig.width=3.5, fig.height=2)
knitr::opts_chunk$set(fig.align = 'center')

```

```{r}
library(ggplot2)
library(dplyr)

diabetes <- read.csv("diabetes.csv", header = TRUE) # dobbiamo tenerli tutti altrimenti perdiamo 2/3 del dataset
diabetes$gender <- factor(diabetes$gender, levels = c("male", "female"), labels = c("male", "female"))
diabetes$location <- factor(diabetes$location, levels = c("Buckingham", "Louisa"), labels = c("Buckingham", "Louisa"))
diabetes$frame <- factor(diabetes$frame, levels = c("small", "medium", "large"), labels = c("small", "medium", "large"))
diabetes$id <- NULL

diabetes$bmi <- (diabetes$weight*0.453592)/((diabetes$height*0.0254)^2)

# Aggiungi la colonna target (1 se glyhb >= 7.0, 0 altrimenti)
diabetes$diagnosis <- ifelse(diabetes$glyhb >= 7.0, 1, 0)

# Converti in fattore (utile per la classificazione)
diabetes$diagnosis <- as.factor(diabetes$diagnosis)
  
```

```{r}
# standardizzare secondo il massimo di colonna.
# oppure Z-score
# analisi univariata
# analisi bivariata
# rimozione missings
num_vars <- diabetes %>% select(where(is.numeric)) %>% names()
factor_vars <- c("gender", "frame", "location")

for (f in factor_vars) {
  for (n in num_vars) {
    p <- ggplot(diabetes, aes_string(x = f, y = n)) +
      geom_boxplot(fill = "#69b3a2", alpha = 0.7) +
      theme_minimal() +
      labs(title = paste("Distribuzione di", n, "per", f),
           x = f, y = n)
    print(p)
  }
}
```
```{r}
library(dplyr)   # Per la manipolazione dei dati

set.seed(123)

# 1. Conversione delle colonne categoriche in fattori
diabetes <- diabetes %>%
  mutate(
    gender = as.factor(gender),
    location = as.factor(location),
    frame = as.factor(frame)
  )

## 1. Sostituisci bp.2* mancanti con bp.1*
diabetes <- diabetes %>% 
  mutate(
    bp.2s_filled = if_else(is.na(bp.2s), bp.1s, bp.2s),
    bp.2d_filled = if_else(is.na(bp.2d), bp.1d, bp.2d)
  )

## 2. Calcola le medie
diabetes <- diabetes %>% 
  mutate(
    bp.s_mean = round((bp.1s + bp.2s_filled) / 2),
    bp.d_mean = round((bp.1d + bp.2d_filled) / 2)
  )

## 3. (Opzionale) rimuovi le colonne intermedie
diabetes <- diabetes %>% 
  select(-bp.2s_filled, -bp.2d_filled)

## Controllo rapido
summary(select(diabetes, bp.1s, bp.2s, bp.s_mean, bp.1d, bp.2d, bp.d_mean))

# Waist-to-Hip Ratio (WHR)
diabetes$whr <- diabetes$waist / diabetes$hip


diabetes_clean <- diabetes %>% 
  # conserva solo le righe in cui TUTTE le colonne tranne altri non hanno NA
  filter(
    if_all(-c(location, age, height, frame, gender, waist, bp.2s, bp.2d, time.ppn), ~ !is.na(.x))
  )

df_model <- diabetes_clean %>%
  select(
    chol,
    stab.glu,
    hdl,
    ratio,
    bmi,
    whr,
    bp.s_mean,
    bp.d_mean,
    diagnosis
  )

write.csv(diabetes_clean, "diabetes_clean.csv")
write.csv(df_model, "diabetes_megapulito.csv")

#Location diagnosis==1 ben distribuita (27 Louisa e 31 Buckingham)
#whr diagnosis==1 è un buon predittore, quelli sopra la media (0.8813459) sono 39 su 58 diabetici del dataset, 
sum(diabetes_clean$diagnosis==1)
sum(diabetes_clean$bp.d_mean >= 75 & diabetes_clean$diagnosis == 1)
sum(diabetes_clean$bp.d_mean < 90 & diabetes_clean$diagnosis == 1)
sum(diabetes_clean$location == "Buckingham" & diabetes_clean$diagnosis == 1)

sum(diabetes_clean$bmi >= 30 & diabetes_clean$diagnosis == 1)

sum(diabetes_clean$bmi >= 30)
sum(diabetes_clean$bmi < 30)

sum(df_model$whr >= 0.8813459 & df_model$diagnosis == 1)

#plsmo in RMS per andamento predittori
#nomogram con modello finale


```


```{r}
# Carica le librerie necessarie
library(caret)
library(dplyr)
library(pROC)
install.packages("rms")

# 1. Shuffle del dataset (mescola le righe)
set.seed(123)  # Per riproducibilità
df_model_shuffled <- df_model %>% 
  sample_frac(size = 1, replace = FALSE)  # Mescola senza ripetizioni
table(df_model_shuffled$diagnosis)

# 2. Split in TRAIN (80%) e TEST (20%) stratificato
train_index <- createDataPartition(
  df_model_shuffled$diagnosis,
  p = 0.8, 
  list = FALSE
)
train_data <- df_model_shuffled[train_index, ]
test_data <- df_model_shuffled[-train_index, ]

# 3. Configura la K-Fold Cross-Validation (es. 10-fold)
ctrl <- trainControl(
  method = "cv",     # Cross-Validation
  number = 15,       # Numero di fold
  savePredictions = "final",
  classProbs = TRUE,  # Necessario per ROC-AUC
  summaryFunction = twoClassSummary  # Se diabetic è factor con livelli "0"/"1"
)

ctrl_boot <- trainControl(
  method = "boot",          # Metodo: Bootstrap resampling
  number = 100,              # Numero di bootstrap samples
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)


# 4. Addestra il modello con K-Fold e Regressione Logistica
# Nota: Converti 'diabetic' in factor con livelli validi per caret
train_data$diagnosis <- make.names(train_data$diagnosis)  # Converti in "X0"/"X1"

logit_model <- train(
  diagnosis ~     chol + stab.glu + stab.glu + bp.s_mean + whr,
  data = train_data,
  method = "glm",        # Regressione Logistica
  family = "binomial",   # Famiglia per classificazione
  trControl = ctrl,
  preProcess = c("center", "scale"),  # Normalizza le variabili
  metric = "ROC"         # Ottimizza per l'AUC
)

# 5. Valutazione sul Test Set
# Prepara il test set
test_data$diagnosis <- make.names(test_data$diagnosis)  # Stessa conversione del training

# Predizioni
test_pred <- predict(logit_model, newdata = test_data)
test_prob <- predict(logit_model, newdata = test_data, type = "prob")[, "X1"]  # Probabilità per classe "1"

test_pred <- factor(test_pred, levels = c("X0", "X1"))
test_data$diagnosis <- factor(test_data$diagnosis, levels = c("X0", "X1"))
table(test_pred)
table(test_data$diagnosis)

## Matrice di confusione
confusionMatrix(
  test_pred,
  test_data$diagnosis,
  positive = "X1"  # Specifica la classe positiva (diabetici)
)

# ROC-AUC
roc_obj <- roc(
  response = as.numeric(test_data$diagnosis == "X1"),
  predictor = test_prob
)
plot(roc_obj, main = "ROC Curve")
auc(roc_obj)

# 6. Output del modello finale
summary(logit_model$finalModel)  # Coefficienti e p-value
varImp(logit_model)             # Importanza delle variabili
```
```{r}
sum(is.na(diabetes$diagnosis))
```
## 1. Feature engineering – WHR e spline
```{r}
# --- NEW BLOCK: ulteriori predittori ---------------------------------
# Waist-to-Hip Ratio (WHR)
diabetes$whr <- diabetes$waist / diabetes$hip

# (facoltativo) quadrato del BMI per eventuale curvatura semplice
diabetes$bmi_sq <- diabetes$bmi^2
# ---------------------------------------------------------------------

```

## 2. Ricetta tidymodels con spline
```{r}
# --- NEW BLOCK: tidymodels pipeline con spline e SMOTE ---------------
library(tidymodels)
set.seed(123)

# dataset finale
df_model_2 <- diabetes_clean %>% 
  select(chol, stab.glu, hdl, ratio, bmi, bmi_sq, whr,
         bp.s_mean, bp.d_mean, diagnosis)

# split stratificato
data_split  <- initial_split(df_model_2, strata = diagnosis, prop = .8)
train_data  <- training(data_split)
test_data   <- testing(data_split)

# ricetta: spline naturali su age e bmi (deg_free = 4), dummy var., SMOTE
rec <- recipe(diagnosis ~ ., data = train_data) %>% 
  step_ns(bmi, deg_free = 4) %>%                 # spline BMI
  step_ns(whr, deg_free = 4) %>%                 # spline WHR
  step_dummy(all_nominal_predictors()) %>% 
  step_smote(diagnosis) %>%                      # bilanciamento
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())

# modello (regressione logistica penalizzata − elastic net)
log_model <- logistic_reg(penalty = tune(), mixture = tune()) %>% 
             set_engine("glmnet")

# griglia di tuning
lambda_grid <- grid_regular(penalty(), mixture(), levels = 5)

# workflow
wf <- workflow() %>% add_model(log_model) %>% add_recipe(rec)

# 10-fold CV
folds <- vfold_cv(train_data, v = 10, strata = diagnosis)

# tuning
set.seed(123)
res <- tune_grid(
  wf, resamples = folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, pr_auc, accuracy)
)

best <- select_best(res, "roc_auc")
final_wf <- finalize_workflow(wf, best)

# fit finale
final_fit <- fit(final_wf, data = train_data)

# performance test-set
test_results <- final_fit %>% 
  predict(test_data, type = "prob") %>% 
  bind_cols(test_data) %>% 
  roc_auc(truth = diagnosis, .pred_1)
test_results
# ---------------------------------------------------------------------

```

## 3
```{r}
# == PRIMA della createDataPartition ==
# (dopo avere aggiunto BMI, WHR, bmi_sq)





```

```{r}
# Carica le librerie necessarie
library(caret)
library(dplyr)
library(pROC)

diabetes_clean_2 <- diabetes %>% 
  # conserva solo le righe in cui TUTTE le colonne tranne altri non hanno NA
  filter(
    if_all(-c(location, hip, age, height, weight, frame, gender, waist, bp.2s, bp.2d, time.ppn), ~ !is.na(.x))
  )

df_model_3 <- diabetes_clean_2 %>%
  select(
    chol, stab.glu, hdl, ratio, bmi, whr, bp.s_mean, bp.d_mean, diagnosis
  )

df_model_3 <- df_model_3 %>% na.omit()       # <-- rimuove le righe con NA

# 1. Shuffle del dataset (mescola le righe)
set.seed(123)  # Per riproducibilità
df_model_shuffled_3 <- df_model_3 %>% 
  sample_frac(size = 1, replace = FALSE)  # Mescola senza ripetizioni
table(df_model_shuffled$diagnosis)

df_model_shuffled_3 <- df_model_3 %>% sample_frac(1)

# 2. Split in TRAIN (80%) e TEST (20%) stratificato
train_index <- createDataPartition(
  df_model_shuffled$diagnosis,
  p = 0.8, 
  list = FALSE
)
train_data_3 <- df_model_shuffled_3[train_index, ]
test_data_3 <- df_model_shuffled_3[-train_index, ]

# 3. Configura la K-Fold Cross-Validation (es. 10-fold)
# == SOSTITUISCI il blocco trainControl ==
ctrl_3 <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "smote"          # <-- bilanciamento automatico
)


# 4. Addestra il modello con K-Fold e Regressione Logistica
# Nota: Converti 'diabetic' in factor con livelli validi per caret
train_data_3$diagnosis <- make.names(train_data_3$diagnosis)  # Converti in "X0"/"X1"

# == SOSTITUISCI formula del train() includendo nuovi predittori ==
logit_model_3 <- train(
  diagnosis ~ chol + stab.glu + hdl + ratio + bmi + whr +
              bp.s_mean + bp.d_mean,
  data = train_data_3,
  method = "glm",
  family = "binomial",
  trControl = ctrl_3,
  preProcess = c("center", "scale"),
  metric = "ROC"
)

# 5. Valutazione sul Test Set
# Prepara il test set
test_data_3$diagnosis <- make.names(test_data_3$diagnosis)  # Stessa conversione del training

# Predizioni
test_pred_3 <- predict(logit_model_3, newdata = test_data_3)
test_prob_3 <- predict(logit_model_3, newdata = test_data_3, type = "prob")[, "X1"]  # Probabilità per classe "1"

test_pred_3 <- factor(test_pred_3, levels = c("X0", "X1"))
test_data_3$diagnosis <- factor(test_data_3$diagnosis, levels = c("X0", "X1"))
table(test_pred_3)
table(test_data_3$diagnosis)

## Matrice di confusione
confusionMatrix(
  test_pred_3,
  test_data_3$diagnosis,
  positive = "X1"  # Specifica la classe positiva (diabetici)
)

# ROC-AUC
roc_obj_3 <- roc(
  response = as.numeric(test_data_3$diagnosis == "X1"),
  predictor = test_prob_3
)
plot(roc_obj_3, main = "ROC Curve")
auc(roc_obj_3)

# 6. Output del modello finale
summary(logit_model_3$finalModel)  # Coefficienti e p-value
varImp(logit_model_3)             # Importanza delle variabili
```

## 4. Calibrazione e decision-curve 

```{r}
# --- Calibrazione -----------------------------------------------------
library(rms)
cal <- val.prob(p = test_prob, y = as.numeric(test_data$diagnosis == "X1"))
print(cal)

# --- Decision Curve ---------------------------------------------------
library(rmda)
dca <- decision_curve(
  diagnosis ~ test_prob,
  data = test_data,
  thresholds = seq(0.05, 0.40, by = 0.05),
  bootstraps = 200              # stima intervalli
)
plot_decision_curve(dca, curve.names = "Logistic model")

```


## 5 Analisi descrittiva da includere nel report
```{r}
# Tabella 1 rapida
library(tableone)
vars <- c("age","gender","location","bmi","whr",
          "chol","hdl","ratio","bp.s_mean","bp.d_mean")
tab1 <- CreateTableOne(vars = vars, strata = "diagnosis", data = diabetes_clean, factorVars = c("gender","location"))
print(tab1, quote = FALSE, noSpaces = TRUE)

# Correlation heatmap
library(GGally)
ggcorr(df_model %>% select(-diagnosis), 
       label = TRUE, label_round = 2, label_size = 3)

```


```{r}
# Carica le librerie necessarie
library(mice)    # Per l'imputazione multivariata (MICE)

# Definisci i metodi per colonna
method <- rep("pmm", ncol(diabetes))
method[colnames(diabetes) == "gender"] <- "logreg"
method[colnames(diabetes) == "location"] <- "polyreg"
method[colnames(diabetes) == "frame"] <- "polr"

imputed_data <- mice(
  diabetes,   # Esclude la prima colonna (id)
  m = 5,
  method = method,  # Predictive Mean Matching (adatto per dati numerici e categorici)
  maxit = 15
)

# 3. Estrai il dataset completo
complete_df <- complete(imputed_data)
complete_df

write.csv(complete_df, "diabetes_mice.csv")

```
